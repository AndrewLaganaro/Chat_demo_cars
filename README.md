# üß† Car Assistant Chat (Demo com LLM local + MCP)

Este projeto √© uma demo m√≠nima que simula um chat entre um cliente e um assistente de carros, que faz consultas a um servidor MCP (Model Context Protocol) e simula um RAG b√°sico obtendo informa√ß√µes de um servidor e usando-as para enriquecer a resposta do chat.

---

## üöó O que este projeto faz?

- O usu√°rio conversa com um assistente via terminal.
- O assistente coleta informa√ß√µes m√≠nimas obrigat√≥rias: **Marca** e **Tipo** do carro (urbano ou estrada).
- Uma requisi√ß√£o √© feita ao servidor **MCP**, que busca todos os carros da mesma marca e tipo.
- O modelo LLM gera uma resposta comentando a tabela recebida e comparando os carros da mesma marca.

Este projeto usa o modelo:

> ‚úÖ `TinyLlama/TinyLlama-1.1B-Chat-v1.0`  
- Pequeno, leve, e instru√≠do para chat ‚Äî ideal para m√°quinas locais com VRAM limitada.
- Infelizmente, teve um desempenho bem decepcionante com o que era esperado dados os chats atuais na web, uma abordagem com API da OpenAI tornaria isso muito mais f√°cil, e os problemas abordados seriam bem menores
- Modelos menores s√£o pouco otimizados para tarefas de chat, eles podem apresentar comportamento repetitivo (repetir uma rotina ou uma hist√≥ria, se ela foi dada num prompt de instru√ß√£o inicial), altera√ß√£o de idioma numa mesma resposta, reproduzir o prompt de instru√ß√£o mesmo quando claramente dito para n√£o fazer isso, e outros problemas que n√£o s√£o esperados em modelos maiores.
- Modelos maiores por outro lado funcionam plenamente em chats, mas exigem uma GPU com pelo menos 16GB de VRAM, o que n√£o √© o caso da maioria dos usu√°rios, nem mesmo do autor deste projeto.
- Uma abordagem com API geraria outras preocupa√ß√µes menores em rela√ß√£o a contexto de resposta de modelo, embora prompts corretos ainda sejam √∫teis, mas daria mais espa√ßo a outros pontos relacionados ao c√≥digo em si, modulariza√ß√£o, classes, aplica√ß√µes RAG de verdade com vetoriza√ß√£o e Vector Stores (como n√£o √© o caso aqui), e outras melhorias em funcionalidades, mas que n√£o s√£o o foco deste projeto.

---

## üì¶ Requisitos

- Python 3.10 ou superior
- CUDA instalado (GPU obrigat√≥ria para rodar localmente o modelo com desempenho aceit√°vel)
- NVIDIA GPU com pelo menos **6GB de VRAM** (recomendada: 8GB+)
- A GPU usada nos testes foi uma **NVIDIA GeForce RTX 3060 Ti**

---

## üíª Setup (passo a passo)

1. **Clone ou baixe este reposit√≥rio**

2. **Crie e ative um ambiente virtual**
```bash
python -m venv .venv
.venv\Scripts\activate
```

3. **Instale as depend√™ncias**
```bash
pip install -r requirements.txt
```
3.1 **Instale o PyTorch com suporte a CUDA**
```bash
https://pytorch.org/get-started/locally/
```

3.2 **Instale o CUDA Toolkit**
    - Siga as instru√ß√µes de instala√ß√£o do [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads) para sua plataforma.

4. **Teste sua GPU com CUDA**
```bash
python test_cuda.py
```
5. **Baixe o modelo de linguagem**
```bash
python retrieve_model.py
```
---

## üõ†Ô∏è Inicializa√ß√£o

1. **Gere o banco de dados falso (roda uma vez s√≥):**
```bash
python generate_fake_db.py
```

2. **Em um terminal separado, inicie o servidor MCP**
```bash
python content_server_mcp.py
```

3. **Em outro terminal, rode o chat com o assistente**
```bash
python client_chat.py
```

---

## üí¨ Exemplos testados

Voc√™ pode testar conversas como:

```text
Quero um carro Mitsubishi urbano
Quero um carro Nissan estrada
```

Os modelos de carros dispon√≠veis s√£o:
> ['Fiat', 'Ford', 'Chevrolet', 'Volkswagen', 'Honda', 'Mitsubishi', 'Toyota', 'Nissan', 'Hyundai', 'Kia']

Os tipos de carros dispon√≠veis s√£o:
> ['urbano', 'estrada']

O assistente ir√° responder com uma tabela formatada (na maioria das vezes) e um coment√°rio comparando os modelos encontrados.(pelo modelo ser pequeno, os coment√°rios n√£o fazem muito sentido, mas o objetivo √© demonstrar a funcionalidade)

---

## ‚ùó Importante

- O projeto roda um **modelo LLM localmente**, o que exige que o **CUDA esteja instalado corretamente** para usar a GPU.
- A execu√ß√£o na CPU √© poss√≠vel, mas **muito mais lenta**.
- O c√≥digo √© uma prova de conceito e pode ser melhorado em robustez, modulariza√ß√£o, tratamento de exce√ß√µes e aplica√ß√£o real de RAG com vetoriza√ß√£o

---

## üõ† Curiosidade t√©cnica

Este projeto simula o uso de um **protocolo MCP** onde o modelo n√£o responde diretamente √†s perguntas, mas sim comenta os resultados de uma consulta intermedi√°ria.

Isso simula cen√°rios reais de:
- integra√ß√£o com bancos de dados
- separa√ß√£o entre motor de busca (MCP) e camada de linguagem natural (LLM)
- aplica√ß√£o de RAG (mesmo que conceitual) dentro do servidor MCP
